{"cells":[{"cell_type":"markdown","metadata":{"id":"bSz5jzj61nHc"},"source":["# BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XyEzhFufN1gA"},"outputs":[],"source":["import math\n","import re\n","from random import *\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F"]},{"cell_type":"markdown","metadata":{"id":"bv4_ZbU8N1gF"},"source":["## 1. Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QYSOA1wrN1gH"},"outputs":[],"source":["import spacy\n","\n","with open(\"../data/wiki_king.txt\", \"r\") as f:\n","    raw_text = f.read()"]},{"cell_type":"markdown","metadata":{"id":"ROzfFGMXN1gL"},"source":["## 2. Preprocessing\n","\n","### Tokenization and numericalization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JrUXZa1VN1gN"},"outputs":[],"source":["import spacy\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(raw_text)\n","sentences = list(doc.sents)\n","\n","#lower case, and clean all the symbols\n","text = [x.text.lower() for x in sentences]\n","text = [re.sub(\"[.,!?\\\\-]\", '', x) for x in text]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"19Ewv-S0N1gP"},"outputs":[],"source":["#making vocabs - numericalization\n","word_list = list(set(\" \".join(text).split()))\n","word2id   = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tYMjy6SjN1gQ"},"outputs":[],"source":["for i, w in enumerate(word_list):\n","    word2id[w] = i + 4 #reserve the first 0-3 for CLS, PAD\n","    id2word    = {i:w for i, w  in enumerate(word2id)}\n","    vocab_size = len(word2id)\n","\n","token_list = list()\n","for sentence in sentences:\n","    arr = [word2id[word] for sentence in text for word in sentence.split()]\n","    token_list.append(arr)"]},{"cell_type":"markdown","metadata":{"id":"5ktEt4mVN1gT"},"source":["## 3. Data loader\n","\n","We gonna make dataloader.  Inside here, we need to make two types of embeddings: **token embedding** and **segment embedding**\n","\n","1. **Token embedding** - Given “The cat is walking. The dog is barking”, we add [CLS] and [SEP] >> “[CLS] the cat is walking [SEP] the dog is barking”.\n","\n","2. **Segment embedding**\n","A segment embedding separates two sentences, i.e., [0 0 0 0 1 1 1 1 ]\n","\n","3. **Masking**\n","As mentioned in the original paper, BERT randomly assigns masks to 15% of the sequence. In this 15%, 80% is replaced with masks, while 10% is replaced with random tokens, and the rest 10% is left as is.  Here we specified `max_pred`\n","\n","4. **Padding**\n","Once we mask, we will add padding. For simplicity, here we padded until some specified `max_len`.\n","\n","Note:  `positive` and `negative` are just simply counts to keep track of the batch size.  `positive` refers to two sentences that are really next to one another."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xHUCkTMEN1gV"},"outputs":[],"source":["batch_size = 6\n","max_mask   = 5 #even though it does not reach 15% yet....maybe you can set this threshold\n","max_len    = 1000 #maximum length that my transformer will accept.....all sentence will be padded"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HH2XqX6_N1gX"},"outputs":[],"source":["def make_batch():\n","    batch = []\n","    positive = negative = 0\n","    while positive != batch_size / 2 or negative != batch_size / 2:\n","\n","        #randomly choose two sentence\n","        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n","        tokens_a, tokens_b            = token_list[tokens_a_index], token_list[tokens_b_index]\n","\n","        #1. token embedding - add CLS and SEP\n","        input_ids = [word2id['[CLS]']] + tokens_a + [word2id['[SEP]']] + tokens_b + [word2id['[SEP]']]\n","\n","        #2. segment embedding - which sentence is 0 and 1\n","        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n","\n","        #3 masking\n","        n_pred = min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n","        #get all the pos excluding CLS and SEP\n","        candidates_masked_pos = [i for i, token in enumerate(input_ids) if token != word2id['[CLS]']\n","                                 and token != word2id['[SEP]']]\n","        shuffle(candidates_masked_pos)\n","        masked_tokens, masked_pos = [], []\n","        #simply loop and mask accordingly\n","        for pos in candidates_masked_pos[:n_pred]:\n","            masked_pos.append(pos)\n","            masked_tokens.append(input_ids[pos])\n","            if random() < 0.1:  #10% replace with random token\n","                index = randint(0, vocab_size - 1)\n","                input_ids[pos] = word2id[id2word[index]]\n","            elif random() < 0.8:  #80 replace with [MASK]\n","                input_ids[pos] = word2id['[MASK]']\n","            else:\n","                pass\n","\n","        #4. pad the sentence to the max length\n","        n_pad = max_len - len(input_ids)\n","        input_ids.extend([0] * n_pad)\n","        segment_ids.extend([0] * n_pad)\n","\n","        #5. pad the mask tokens to the max length\n","        if max_mask > n_pred:\n","            n_pad = max_mask - n_pred\n","            masked_tokens.extend([0] * n_pad)\n","            masked_pos.extend([0] * n_pad)\n","\n","        #6. check whether is positive or negative\n","        if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n","            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n","            positive += 1\n","        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size / 2:\n","            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n","            negative += 1\n","\n","    return batch\n",""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B2GuMGBFN1ga"},"outputs":[],"source":["batch = make_batch()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tOubwhpuN1gc"},"outputs":[],"source":["len(batch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nf4ZJPuLN1ge"},"outputs":[],"source":["input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8nP1qm4oN1gg"},"outputs":[],"source":["input_ids.shape, segment_ids.shape, masked_tokens.shape, masked_pos.shape, isNext"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jvmiPvXON1gi"},"outputs":[],"source":["masked_tokens"]},{"cell_type":"markdown","metadata":{"id":"vxDAir5ON1gj"},"source":["## 4. Model\n","\n","Recall that BERT only uses the encoder.\n","\n","BERT has the following components:\n","\n","- Embedding layers\n","- Attention Mask\n","- Encoder layer\n","- Multi-head attention\n","- Scaled dot product attention\n","- Position-wise feed-forward network\n","- BERT (assembling all the components)"]},{"cell_type":"markdown","metadata":{"id":"xPxxmAmFN1gk"},"source":["## 4.1 Embedding\n","\n","<img src = \"../figures/BERT_embed.png\" width=500>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C-ObQjetN1gl"},"outputs":[],"source":["class Embedding(nn.Module):\n","    def __init__(self):\n","        super(Embedding, self).__init__()\n","        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n","        self.pos_embed = nn.Embedding(max_len, d_model)      # position embedding\n","        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n","        self.norm = nn.LayerNorm(d_model)\n","\n","    def forward(self, x, seg):\n","        #x, seg: (bs, len)\n","        seq_len = x.size(1)\n","        pos = torch.arange(seq_len, dtype=torch.long)\n","        pos = pos.unsqueeze(0).expand_as(x)  # (len,) -> (bs, len)\n","        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n","        return self.norm(embedding)"]},{"cell_type":"markdown","metadata":{"id":"ApID66iwN1gm"},"source":["## 4.2 Attention mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s1PGksqBNuZM"},"outputs":[],"source":["def get_attn_pad_mask(seq_q, seq_k):\n","    batch_size, len_q = seq_q.size()\n","    batch_size, len_k = seq_k.size()\n","    # eq(zero) is PAD token\n","    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n","    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"]},{"cell_type":"markdown","metadata":{"id":"M9fLHLdpN1go"},"source":["### Testing the attention mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N3BzrvALN1go"},"outputs":[],"source":["print(get_attn_pad_mask(input_ids, input_ids).shape)"]},{"cell_type":"markdown","metadata":{"id":"kAsKrzl2N1gp"},"source":["## 4.3 Encoder\n","\n","The encoder has two main components:\n","\n","- Multi-head Attention\n","- Position-wise feed-forward network\n","\n","First let's make the wrapper called `EncoderLayer`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iPd2wbMzN1gp"},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","    def __init__(self):\n","        super(EncoderLayer, self).__init__()\n","        self.enc_self_attn = MultiHeadAttention()\n","        self.pos_ffn       = PoswiseFeedForwardNet()\n","\n","    def forward(self, enc_inputs, enc_self_attn_mask):\n","        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n","        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n","        return enc_outputs, attn"]},{"cell_type":"markdown","metadata":{"id":"HIsvWjXsN1gq"},"source":["Let's define the scaled dot attention, to be used inside the multihead attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xoCz6Hi6N1gr"},"outputs":[],"source":["class ScaledDotProductAttention(nn.Module):\n","    def __init__(self):\n","        super(ScaledDotProductAttention, self).__init__()\n","\n","    def forward(self, Q, K, V, attn_mask):\n","        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n","        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n","        attn = nn.Softmax(dim=-1)(scores)\n","        context = torch.matmul(attn, V)\n","        return context, attn"]},{"cell_type":"markdown","metadata":{"id":"_zTZa6wNN1gr"},"source":["Let's define the parameters first"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZ0sYMTyN1gs"},"outputs":[],"source":["n_layers = 6    # number of Encoder of Encoder Layer\n","n_heads  = 8    # number of heads in Multi-Head Attention\n","d_model  = 768  # Embedding Size\n","d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n","d_k = d_v = 64  # dimension of K(=Q), V\n","n_segments = 2"]},{"cell_type":"markdown","metadata":{"id":"tno8q7cEN1gs"},"source":["Here is the Multiheadattention."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HuQHP_LIN1gt"},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self):\n","        super(MultiHeadAttention, self).__init__()\n","        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n","        self.W_K = nn.Linear(d_model, d_k * n_heads)\n","        self.W_V = nn.Linear(d_model, d_v * n_heads)\n","    def forward(self, Q, K, V, attn_mask):\n","        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n","        residual, batch_size = Q, Q.size(0)\n","        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n","        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n","        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n","        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n","\n","        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n","\n","        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n","        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n","        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n","        output = nn.Linear(n_heads * d_v, d_model)(context)\n","        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n"]},{"cell_type":"markdown","metadata":{"id":"L_bQfk6kN1gu"},"source":["Here is the PoswiseFeedForwardNet."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBjZYXJEN1gu"},"outputs":[],"source":["class PoswiseFeedForwardNet(nn.Module):\n","    def __init__(self):\n","        super(PoswiseFeedForwardNet, self).__init__()\n","        self.fc1 = nn.Linear(d_model, d_ff)\n","        self.fc2 = nn.Linear(d_ff, d_model)\n","\n","    def forward(self, x):\n","        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n","        return self.fc2(F.gelu(self.fc1(x)))\n"]},{"cell_type":"markdown","metadata":{"id":"V_H0xmvAN1gv"},"source":["## 4.4 Putting them together"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OZ0TJ84W4SZw"},"outputs":[],"source":["class BERT(nn.Module):\n","    def __init__(self):\n","        super(BERT, self).__init__()\n","        self.embedding = Embedding()\n","        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n","        self.fc = nn.Linear(d_model, d_model)\n","        self.activ = nn.Tanh()\n","        self.linear = nn.Linear(d_model, d_model)\n","        self.norm = nn.LayerNorm(d_model)\n","        self.classifier = nn.Linear(d_model, 2)\n","        # decoder is shared with embedding layer\n","        embed_weight = self.embedding.tok_embed.weight\n","        n_vocab, n_dim = embed_weight.size()\n","        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n","        self.decoder.weight = embed_weight\n","        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n","\n","    def forward(self, input_ids, segment_ids, masked_pos):\n","        output = self.embedding(input_ids, segment_ids)\n","        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n","        for layer in self.layers:\n","            output, enc_self_attn = layer(output, enc_self_attn_mask)\n","        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n","\n","        # 1. predict next sentence\n","        # it will be decided by first token(CLS)\n","        h_pooled   = self.activ(self.fc(output[:, 0])) # [batch_size, d_model]\n","        logits_nsp = self.classifier(h_pooled) # [batch_size, 2]\n","\n","        # 2. predict the masked token\n","        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n","        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n","        h_masked  = self.norm(F.gelu(self.linear(h_masked)))\n","        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n","\n","        return logits_lm, logits_nsp"]},{"cell_type":"markdown","metadata":{"id":"Y0HwALdbN1gw"},"source":["## 5. Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8UAG3SEP4UbU"},"outputs":[],"source":["num_epoch = 500\n","model = BERT()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","batch = make_batch()\n","input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n","\n","for epoch in range(num_epoch):\n","    optimizer.zero_grad()\n","    logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)\n","    #logits_lm: (bs, max_mask, vocab_size) ==> (6, 5, 34)\n","    #logits_nsp: (bs, yes/no) ==> (6, 2)\n","\n","    #1. mlm loss\n","    #logits_lm.transpose: (bs, vocab_size, max_mask) vs. masked_tokens: (bs, max_mask)\n","    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n","    loss_lm = (loss_lm.float()).mean()\n","    #2. nsp loss\n","    #logits_nsp: (bs, 2) vs. isNext: (bs, )\n","    loss_nsp = criterion(logits_nsp, isNext) # for sentence classification\n","\n","    #3. combine loss\n","    loss = loss_lm + loss_nsp\n","    if epoch % 100 == 0:\n","        print('Epoch:', '%02d' % (epoch), 'loss =', '{:.6f}'.format(loss))\n","    loss.backward()\n","    optimizer.step()"]},{"cell_type":"markdown","metadata":{"id":"mh1_1fZtN1gy"},"source":["## 6. Inference\n","\n","Since our dataset is very small, it won't work very well, but just for the sake of demonstration."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uD3K8T6B4YJp"},"outputs":[],"source":["# Predict mask tokens ans isNext\n","input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[2]))\n","print([id2word[w.item()] for w in input_ids[0] if id2word[w.item()] != '[PAD]'])\n","\n","logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)\n","#logits_lm:  (1, max_mask, vocab_size) ==> (1, 5, 34)\n","#logits_nsp: (1, yes/no) ==> (1, 2)\n","\n","#predict masked tokens\n","#max the probability along the vocab dim (2), [1] is the indices of the max, and [0] is the first value\n","logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n","#note that zero is padding we add to the masked_tokens\n","print('masked tokens (words) : ',[id2word[pos.item()] for pos in masked_tokens[0]])\n","print('masked tokens list : ',[pos.item() for pos in masked_tokens[0]])\n","print('masked tokens (words) : ',[id2word[pos.item()] for pos in logits_lm])\n","print('predict masked tokens list : ', [pos for pos in logits_lm])\n","\n","#predict nsp\n","logits_nsp = logits_nsp.data.max(1)[1][0].data.numpy()\n","print(logits_nsp)\n","print('isNext : ', True if isNext else False)\n","print('predict isNext : ',True if logits_nsp else False)"]},{"cell_type":"markdown","metadata":{"id":"PM5UrdYoN1hC"},"source":["Trying a bigger dataset should be able to see the difference."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":0}