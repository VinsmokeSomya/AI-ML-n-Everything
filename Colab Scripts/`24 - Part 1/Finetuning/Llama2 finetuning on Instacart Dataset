{"cells":[{"cell_type":"markdown","metadata":{"id":"C2EgqEPDQ8v6"},"source":["## Installing Necessary Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mNnkgBq7Q3EU"},"outputs":[],"source":["!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n","!pip install -q datasets bitsandbytes einops wandb"]},{"cell_type":"markdown","source":["# Dataset details\n","Instacart Data can be downloaded from [here](https://www.kaggle.com/competitions/instacart-market-basket-analysis/data). We just need product & department csv files"],"metadata":{"id":"e2OTXLba3bnJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6opcW_KjR8j9"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e4fUrzTUTyOK"},"outputs":[],"source":["!ls /content/drive/MyDrive/'Colab Notebooks'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KJmHtRksTUmv"},"outputs":[],"source":["import pandas as pd\n","df_product = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/products.csv\")\n","df_dept = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/departments.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WHWZSGxsUSkX"},"outputs":[],"source":["df_joined = pd.merge(df_product, df_dept, on = ['department_id'])\n","df_joined['text'] = df_joined.apply(lambda row: row['product_name'] + \" ->: \" + row['department'], axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GkmdTDF0Wc3e"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","train_df, test_df = train_test_split(df_joined, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"le1r4sDSWjba"},"outputs":[],"source":["train_df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sYD0jpyYWr4D"},"outputs":[],"source":["test_df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I1BL-ToXXbaI"},"outputs":[],"source":["from datasets import Dataset,DatasetDict\n","train_dataset_dict = DatasetDict({\n","    \"train\": Dataset.from_pandas(train_df),\n","})"]},{"cell_type":"markdown","metadata":{"id":"rjOMoSbGSxx9"},"source":["## Loading the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZwXZbQ2dSwzI"},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n","\n","model_name = \"TinyPixel/Llama-2-7B-bf16-sharded\"\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16,\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    trust_remote_code=True\n",")\n","model.config.use_cache = False"]},{"cell_type":"markdown","metadata":{"id":"xNqIYtQcUBSm"},"source":["Let's also load the tokenizer below"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDS2yYmlUAD6"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"markdown","source":["**Let's check what the base model predicts before finetuning. :)**"],"metadata":{"id":"7ihwFhZt4IlD"}},{"cell_type":"code","source":["import transformers\n","pipeline = transformers.pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    torch_dtype=torch.bfloat16,\n","    trust_remote_code=True,\n","    device_map=\"auto\",\n",")\n","\n","\n","sequences = pipeline(\n","   [\"“Free & Clear Stage 4 Overnight Diapers” ->:\",\"Bread Rolls ->:\",\"French Milled Oval Almond Gourmande Soap ->:\"],\n","    max_length=200,\n","    do_sample=True,\n","    top_k=10,\n","    num_return_sequences=1,\n","    eos_token_id=tokenizer.eos_token_id,\n",")\n","for seq in sequences:\n","    print(f\"Result: {seq[0]['generated_text']}\")"],"metadata":{"id":"9L7Fr5x7EBMQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NuAx3zBeUL1q"},"source":["Below we will load the configuration file in order to create the LoRA model. According to QLoRA paper, it is important to consider all linear layers in the transformer block for maximum performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQdvjTYTT1vQ"},"outputs":[],"source":["from peft import LoraConfig\n","\n","lora_alpha = 16\n","lora_dropout = 0.1\n","lora_r = 64\n","\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules=[\"q_proj\",\"v_proj\"]\n",")"]},{"cell_type":"markdown","metadata":{"id":"dzsYHLwIZoLm"},"source":["## Loading the trainer"]},{"cell_type":"markdown","metadata":{"id":"aTBJVE4PaJwK"},"source":["Here we will use the [`SFTTrainer` from TRL library](https://huggingface.co/docs/trl/main/en/sft_trainer) that gives a wrapper around transformers `Trainer` to easily fine-tune models on instruction based datasets using PEFT adapters. Let's first load the training arguments below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OCFTvGW6aspE"},"outputs":[],"source":["from transformers import TrainingArguments\n","\n","output_dir = \"./results\"\n","per_device_train_batch_size = 4\n","gradient_accumulation_steps = 4\n","optim = \"paged_adamw_32bit\"\n","save_steps = 10\n","logging_steps = 1\n","learning_rate = 2e-4\n","max_grad_norm = 0.3\n","max_steps = 120\n","warmup_ratio = 0.03\n","lr_scheduler_type = \"constant\"\n","\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    fp16=True,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=True,\n","    lr_scheduler_type=lr_scheduler_type,\n",")"]},{"cell_type":"markdown","metadata":{"id":"I3t6b2TkcJwy"},"source":["Then finally pass everthing to the trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TNeOBgZeTl2H"},"outputs":[],"source":["from trl import SFTTrainer\n","\n","max_seq_length = 512\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=train_dataset_dict['train'],\n","    # train_dataset=data['train'],\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    # dataset_text_field=\"prediction\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n",")"]},{"cell_type":"markdown","metadata":{"id":"GWplqqDjb3sS"},"source":["We will also pre-process the model by upcasting the layer norms in float 32 for more stable training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7OyIvEx7b1GT"},"outputs":[],"source":["for name, module in trainer.model.named_modules():\n","    if \"norm\" in name:\n","        module = module.to(torch.float32)"]},{"cell_type":"markdown","metadata":{"id":"1JApkSrCcL3O"},"source":["## Train the model"]},{"cell_type":"markdown","metadata":{"id":"JjvisllacNZM"},"source":["Now let's train the model! Simply call `trainer.train()`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_kbS7nRxcMt7"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","source":["lst_test_data = list(test_df['text'])"],"metadata":{"id":"E2tFZUKII4gL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(lst_test_data)"],"metadata":{"id":"QXFR60GMvkZr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_size = 25\n","lst_test_data_short = lst_test_data[:sample_size]"],"metadata":{"id":"XizZTnNivpFx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import transformers\n","\n","pipeline = transformers.pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    # torch_dtype=torch.bfloat16,\n","    torch_dtype=torch.float16,\n","    trust_remote_code=True,\n","    device_map=\"auto\",\n",")\n","\n","sequences = pipeline(\n","    lst_test_data_short,\n","    max_length=100,  #200,\n","    do_sample=True,\n","    top_k=10,\n","    num_return_sequences=1,\n","    eos_token_id=tokenizer.eos_token_id,\n",")\n","\n","for ix,seq in enumerate(sequences):\n","    print(ix,seq[0]['generated_text'])"],"metadata":{"id":"6ob_VSuyBkZF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cUQFjhUcDSZy"},"outputs":[],"source":["def correct_answer(ans):\n","  return (ans.split(\"->:\")[1]).strip()\n","\n","answers = []\n","for ix,seq in enumerate(sequences):\n","    # print(ix,seq[0]['generated_text'])\n","    answers.append(correct_answer(seq[0]['generated_text']))\n","\n","answers"]},{"cell_type":"code","source":["df_evaluate = test_df.iloc[:sample_size][['product_name','department']]\n","\n","df_evaluate = df_evaluate.reset_index(drop=True)\n","\n","df_evaluate['department_predicted'] = answers\n","\n","df_evaluate"],"metadata":{"id":"wzDG4gojQqYv"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"14pj2WaDH8gllQh3piJMoDtz6Vt5CXP7Z","timestamp":1708260510582},{"file_id":"1VEtgjRusMsLLhTslNxKscmSgd0ErlN_f","timestamp":1690048841707},{"file_id":"120OpwCucqTawdRb3IALuVzlURUu-AcKb","timestamp":1689782129366},{"file_id":"1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o","timestamp":1689710853570}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}