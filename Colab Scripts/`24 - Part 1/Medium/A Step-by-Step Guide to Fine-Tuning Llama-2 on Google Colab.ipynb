{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNAuBckRbV37AU5yIEqYT1F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Install required packages**"],"metadata":{"id":"xmJlIB02ng38"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pNrEtaommmjO"},"outputs":[],"source":["!pip install -q -U git+https://github.com/huggingface/transformers.git\n","!pip install -q -U git+https://github.com/huggingface/peft.git\n","!pip install -q -U git+https://github.com/huggingface/accelerate.git\n","!pip install -q trl xformers wandb datasets einops gradio sentencepiece bitsandbytes"]},{"cell_type":"markdown","source":["# **Import required library**"],"metadata":{"id":"qns8TsqVnueb"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging, TextStreamer\n","from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n","import os,torch, wandb, platform, gradio, warnings\n","from datasets import load_dataset\n","from trl import SFTTrainer\n","from huggingface_hub import notebook_login"],"metadata":{"id":"sETKhkV_moop"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Check system spec**"],"metadata":{"id":"mGNri4_Vnw4y"}},{"cell_type":"code","source":["def print_system_specs():\n","    # Check if CUDA is available\n","    is_cuda_available = torch.cuda.is_available()\n","    print(\"CUDA Available:\", is_cuda_available)\n","# Get the number of available CUDA devices\n","    num_cuda_devices = torch.cuda.device_count()\n","    print(\"Number of CUDA devices:\", num_cuda_devices)\n","    if is_cuda_available:\n","        for i in range(num_cuda_devices):\n","            # Get CUDA device properties\n","            device = torch.device('cuda', i)\n","            print(f\"--- CUDA Device {i} ---\")\n","            print(\"Name:\", torch.cuda.get_device_name(i))\n","            print(\"Compute Capability:\", torch.cuda.get_device_capability(i))\n","            print(\"Total Memory:\", torch.cuda.get_device_properties(i).total_memory, \"bytes\")\n","    # Get CPU information\n","    print(\"--- CPU Information ---\")\n","    print(\"Processor:\", platform.processor())\n","    print(\"System:\", platform.system(), platform.release())\n","    print(\"Python Version:\", platform.python_version())\n","print_system_specs()"],"metadata":{"id":"veiaPvtcmrWd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Setting the model variable**"],"metadata":{"id":"Yz6CHuNyn8Pj"}},{"cell_type":"code","source":["# Pre trained model\n","model_name = \"meta-llama/Llama-2-7b-hf\"\n","\n","# Dataset name\n","dataset_name = \"vicgalle/alpaca-gpt4\"\n","\n","# Hugging face repository link to save fine-tuned model(Create new repository in huggingface,copy and paste here)\n","new_model = \"Repository link here\""],"metadata":{"id":"NMM9FoVUmueJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Log into hugging face hub**"],"metadata":{"id":"GTKPRRuFn-TJ"}},{"cell_type":"code","source":["notebook_login()"],"metadata":{"id":"BXX6EcqBmyF_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Load dataset**"],"metadata":{"id":"VtH-Kfq5oBxg"}},{"cell_type":"code","source":["# Load dataset (you can process it here)\n","dataset = load_dataset(dataset_name, split=\"train[0:10000]\")\n","dataset[\"text\"][0]"],"metadata":{"id":"CAqTmI4-m0IN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Loading the model and tokenizer**"],"metadata":{"id":"wiEWmmx4oFki"}},{"cell_type":"code","source":["# Load base model(llama-2-7b-hf) and tokenizer\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit= True,\n","    bnb_4bit_quant_type= \"nf4\",\n","    bnb_4bit_compute_dtype= torch.float16,\n","    bnb_4bit_use_double_quant= False,\n",")\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map={\"\": 0}\n",")\n","model = prepare_model_for_kbit_training(model)\n","model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n","model.config.pretraining_tp = 1\n","# Load LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.add_eos_token = True\n","tokenizer.add_bos_token, tokenizer.add_eos_token"],"metadata":{"id":"4l9YXX4Mm3wP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Monitoring**"],"metadata":{"id":"XLUYb38MoLb5"}},{"cell_type":"code","source":["# #monitering login\n","# wandb.login(key=\"Enter the Authorization code here\")\n","# run = wandb.init(project='Fine tuning llama-2-7B', job_type=\"training\", anonymous=\"allow\")"],"metadata":{"id":"RIPGhYUvm6j9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Lora config**"],"metadata":{"id":"ifIPo4NvoM9r"}},{"cell_type":"code","source":["peft_config = LoraConfig(\n","    lora_alpha= 8,\n","    lora_dropout= 0.1,\n","    r= 16,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\"]\n",")"],"metadata":{"id":"DwO13ZKjm8oz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Training arguments**"],"metadata":{"id":"WkGg3rsYoPxN"}},{"cell_type":"code","source":["training_arguments = TrainingArguments(\n","    output_dir= \"./results\",\n","    num_train_epochs= 1,\n","    per_device_train_batch_size= 8,\n","    gradient_accumulation_steps= 2,\n","    optim = \"paged_adamw_8bit\",\n","    save_steps= 1000,\n","    logging_steps= 30,\n","    learning_rate= 2e-4,\n","    weight_decay= 0.001,\n","    fp16= False,\n","    bf16= False,\n","    max_grad_norm= 0.3,\n","    max_steps= -1,\n","    warmup_ratio= 0.3,\n","    group_by_length= True,\n","    lr_scheduler_type= \"linear\",\n","    report_to=\"wandb\",\n",")"],"metadata":{"id":"AHpESh9lm-oT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **SFTT Trainer arguments**"],"metadata":{"id":"QKnwLsZEoS1z"}},{"cell_type":"code","source":["# Setting sft parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    max_seq_length= None,\n","    dataset_text_field=\"text\",\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing= False,\n",")"],"metadata":{"id":"XmIhcVN8nAhb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Set to begin the training process**"],"metadata":{"id":"p_Q30smIoan2"}},{"cell_type":"code","source":["# Train model\n","trainer.train()"],"metadata":{"id":"B0RuqMcvnCbH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **What after training ?**"],"metadata":{"id":"xfymsRYApBvJ"}},{"cell_type":"code","source":["# Save the fine-tuned model\n","trainer.model.save_pretrained(new_model)\n","wandb.finish()\n","model.config.use_cache = True\n","model.eval()"],"metadata":{"id":"JrjH6MUDpDFn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Letâ€™s test the model**"],"metadata":{"id":"WysYHovTpHvy"}},{"cell_type":"code","source":["def stream(user_prompt):\n","    runtimeFlag = \"cuda:0\"\n","    system_prompt = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n'\n","    B_INST, E_INST = \"### Instruction:\\n\", \"### Response:\\n\"\n","\n","    prompt = f\"{system_prompt}{B_INST}{user_prompt.strip()}\\n\\n{E_INST}\"\n","\n","    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n","\n","    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n","\n","    # Despite returning the usual output, the streamer will also print the generated text to stdout.\n","    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=500)"],"metadata":{"id":"qitlUPKMpFUj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stream(\"what is newtons 3rd law and its formula\")"],"metadata":{"id":"gGXjZz69pMBA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Upload a model to hugging face repository**"],"metadata":{"id":"5aBWdyvwpSDU"}},{"cell_type":"markdown","source":["Step 1: After completing the training of your model, employing the provided code to release this memory becomes crucial. This action is significant as it aids in preventing your computer from facing memory shortages and can also enhance the performance of other concurrently running programs."],"metadata":{"id":"XLE-IeX7pbi2"}},{"cell_type":"code","source":["# Clear the memory footprint\n","del model, trainer\n","torch.cuda.empty_cache()"],"metadata":{"id":"WxMqObYCpTbr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 2: The subsequent step involves merging the adapter with the model."],"metadata":{"id":"p7ueLxAEpdK0"}},{"cell_type":"code","source":["base_model = AutoModelForCausalLM.from_pretrained(\n","    model_name, low_cpu_mem_usage=True,\n","    return_dict=True,torch_dtype=torch.float16,\n","    device_map= {\"\": 0})\n","model = PeftModel.from_pretrained(base_model, new_model)\n","model = model.merge_and_unload()\n","\n","# Reload tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""],"metadata":{"id":"4pUcU0bIpesd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 3: Finally, once the merger is complete, the next action involves pushing the merged model to the Hugging Face hub. This process facilitates the sharing and accessibility of the model for others in the community."],"metadata":{"id":"dXH3jdEEphDj"}},{"cell_type":"code","source":["model.push_to_hub(new_model)\n","tokenizer.push_to_hub(new_model)"],"metadata":{"id":"De1lUS2RpinL"},"execution_count":null,"outputs":[]}]}