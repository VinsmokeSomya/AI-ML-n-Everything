{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/ai-bites/generative-ai-course/blob/main/Finetune_Gemma.ipynb","timestamp":1709690398378}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Step 0 - Premiminaries\n","* Install needed packages\n","* Setup HF ecosystem"],"metadata":{"id":"yWJhJKWnSui3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3WjLPjwhSj2k"},"outputs":[],"source":["# For any HF basic activities like loading models\n","# and tokenizers for running inference\n","# upgrade is a must for the newest Gemma model\n","!pip install --upgrade datasets\n","!pip install --upgrade transformers\n","\n","# For doing efficient stuff - PEFT\n","!pip install --upgrade peft\n","!pip install --upgrade trl\n","!pip install bitsandbytes\n","!pip install accelerate\n","\n","# for logging and visualizing training progress\n","!pip install tensorboard\n","# If creating a new dataset, useful for creating *.jsonl files\n","!pip install jsonlines"]},{"cell_type":"markdown","source":["## Login to HF ecosystem or get the token from secrets"],"metadata":{"id":"y89ErlTLTw1F"}},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","from google.colab import userdata\n","hf_token = userdata.get('hftoken')\n","notebook_login()"],"metadata":{"id":"vR5Ur69BTptY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 1 - Run Inference on Pre-trained Model\n","* Load the model\n","* Load the tokenizer\n","* Visualize the model architecture\n","* Finally, query the model with a prompt and see the response"],"metadata":{"id":"PZgIWadaS8QF"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","\n","model_name = \"google/gemma-2b\"\n","# model_name = \"meta-llama/Llama-2-7b-hf\"\n","\n","model = AutoModelForCausalLM.from_pretrained(model_name,\n","                                            #  torch_dtype=torch.float16\n","                                             )\n","tokenizer = AutoTokenizer.from_pretrained(model_name,\n","                                          # torch_dtype=torch.float16\n","                                          )\n","print(model)"],"metadata":{"id":"X3EcUyo-Yiu0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_text = \"What should I do on a trip to Europe?\"\n","\n","input_ids = tokenizer(input_text, return_tensors=\"pt\")\n","outputs = model.generate(**input_ids, max_length=128)\n","print(tokenizer.decode(outputs[0]))"],"metadata":{"id":"vvWyqC_-SzkF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_text = \"Explain the process of photosynthesis in a way that a child could understand\"\n","input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n","print(input_ids)\n","outputs = model.generate(**input_ids, max_length=128)\n","print(tokenizer.decode(outputs[0]))"],"metadata":{"id":"U3QgLh40gyS9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 2 - Motivation for PEFT\n","- Try to finetune Gemma on Dolly dataset without LoRA\n","  * Load and visualize the dataset\n","  * Initiate the trainer\n","  * Start the training\n","  * Note that its impossible to fine-tune on a single 14GB GPU (T4) on colab"],"metadata":{"id":"v2c0DDNHe7ca"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset_name = \"databricks/databricks-dolly-15k\"\n","dataset = load_dataset(dataset_name, split=\"train[0:1000]\")\n","\n","print(f\"Instruction is: {dataset[0]['instruction']}\")\n","print(f\"Response is: {dataset[0]['response']}\")\n","dataset"],"metadata":{"id":"TRrFcdB2fESh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def formatting_prompts_func(example):\n","  output_texts = []\n","  for i in range(len(example['instruction'])):\n","      if example['category'][i] in ['open_qa', 'general_qa']:\n","        text = f\"Instruction:\\n{example['instruction']}\\n\\nResponse:\\n{example['response']}\"\n","        output_texts.append(text)\n","  return output_texts\n","\n","trainer = SFTTrainer(\n","    model,\n","    train_dataset=dataset,\n","    tokenizer=tokenizer,\n","    formatting_func=formatting_prompts_func,\n",")\n","print(\"Initialized trainer for training!\")\n","\n","trainer.train()"],"metadata":{"id":"EmjBoc1M_h40"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Finetune using Parameter Efficient Finetuning (PEFT)\n","- Create your own dataset\n","  - Visualizing the dataset\n","  - Cleaning and Preprocessing\n","  - Uploading to HF hub\n","- Fine-tune with the dataset\n"],"metadata":{"id":"e_DcK6YbSpF1"}},{"cell_type":"markdown","source":["## Do all the imports"],"metadata":{"id":"1X9yViBR1Ak5"}},{"cell_type":"code","source":["import torch\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer\n"],"metadata":{"id":"mmrCxM7-FAbS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create your own dataset"],"metadata":{"id":"psrAA61t2yZY"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset_name = \"databricks/databricks-dolly-15k\"\n","dataset = load_dataset(dataset_name, split=\"train\")"],"metadata":{"id":"0-Vp19qp_muo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import defaultdict\n","\n","categries_count = defaultdict(int)\n","for __, data in enumerate(dataset):\n","    categries_count[data['category']] += 1\n","print(categries_count)"],"metadata":{"id":"dypFOk2-_0nJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# filter out those that do not have any context\n","filtered_dataset = []\n","for __, data in enumerate(dataset):\n","    if data[\"context\"]:\n","        continue\n","    else:\n","        text = f\"Instruction:\\n{data['instruction']}\\n\\nResponse:\\n{data['response']}\"\n","        filtered_dataset.append({\"text\": text})\n","\n","print(filtered_dataset[0:2])"],"metadata":{"id":"OamKS2Dm_7V0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# convert to json and save the filtered dataset as jsonl file\n","import jsonlines as jl\n","with jl.open('dolly-mini-train.jsonl', 'w') as writer:\n","    writer.write_all(filtered_dataset[0:])\n"],"metadata":{"id":"6goRn3Nu_-qr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset_name = \"ai-bites/databricks-mini\"\n","dataset = load_dataset(dataset_name, split=\"train[0:1000]\")\n","dataset"],"metadata":{"id":"u9KBC0OW20kN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define all the parameters\n","- LoRA parameters\n","- bitsandbytes parameters\n","- training arguments / parameters\n","- Supervised fine-tuning (SFT) parameters"],"metadata":{"id":"azfjilit1QZ1"}},{"cell_type":"code","source":["# define some variables - model names\n","model_name = \"google/gemma-2b\"\n","new_model = \"gemma-ft\"\n","\n","################################################################################\n","# LoRA parameters\n","################################################################################\n","# LoRA attention dimension\n","# lora_r = 64\n","lora_r = 4\n","# Alpha parameter for LoRA scaling\n","lora_alpha = 16\n","# Dropout probability for LoRA layers\n","lora_dropout = 0.1\n","\n","################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_nested_quant = False\n","\n","################################################################################\n","# TrainingArguments parameters\n","################################################################################\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = \"./results\"\n","# Number of training epochs\n","num_train_epochs = 1\n","# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = False\n","bf16 = False\n","# Batch size per GPU for training\n","per_device_train_batch_size = 4\n","# Batch size per GPU for evaluation\n","per_device_eval_batch_size = 4\n","# Number of update steps to accumulate the gradients for\n","gradient_accumulation_steps = 1\n","# Enable gradient checkpointing\n","gradient_checkpointing = True\n","# Maximum gradient normal (gradient clipping)\n","max_grad_norm = 0.3\n","# Initial learning rate (AdamW optimizer)\n","learning_rate = 2e-4\n","# Weight decay to apply to all layers except bias/LayerNorm weights\n","weight_decay = 0.001\n","# Optimizer to use\n","optim = \"paged_adamw_32bit\"\n","# Learning rate schedule (constant a bit better than cosine)\n","lr_scheduler_type = \"constant\"\n","# Number of training steps (overrides num_train_epochs)\n","max_steps = -1\n","# Ratio of steps for a linear warmup (from 0 to learning rate)\n","warmup_ratio = 0.03\n","# Group sequences into batches with same length\n","# Saves memory and speeds up training considerably\n","group_by_length = True\n","# Save checkpoint every X updates steps\n","save_steps = 25\n","# Log every X updates steps\n","logging_steps = 25\n","\n","################################################################################\n","# SFT parameters\n","################################################################################\n","# Maximum sequence length to use\n","max_seq_length = 40 # None\n","# Pack multiple short examples in the same input sequence to increase efficiency\n","packing = True # False\n","# Load the entire model on the GPU 0\n","# device_map = {\"\": 0}\n","device_map=\"auto\"\n"],"metadata":{"id":"0y4Yvw6FS8f1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load QLoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit, # Activates 4-bit precision loading\n","    bnb_4bit_quant_type=bnb_4bit_quant_type, # nf4\n","    bnb_4bit_compute_dtype=compute_dtype, # float16\n","    bnb_4bit_use_double_quant=use_nested_quant, # False\n",")"],"metadata":{"id":"8DtwgVNS2oda"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check GPU compatibility with bfloat16\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"Setting BF16 to True\")\n","        bf16 = True\n","    else:\n","        bf16 = False\n"],"metadata":{"id":"dXLXjahA3E8_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load the Model and Tokenizer"],"metadata":{"id":"rh7fRGMj3HaE"}},{"cell_type":"code","source":["# Load base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    token=hf_token,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name,\n","                                          token=hf_token,\n","                                          trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n"],"metadata":{"id":"9WdymvXh3hgl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\"]\n",")"],"metadata":{"id":"7wU0mM5i3gBT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    report_to=\"tensorboard\",\n",")\n","training_arguments"],"metadata":{"id":"iTHsgDg23U7-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    # formatting_func=format_prompts_fn,\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")"],"metadata":{"id":"jOQWWdYQ3VrG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train model\n","trainer.train()\n","trainer.model.save_pretrained(new_model)"],"metadata":{"id":"GJawdXGQ3VZj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualize training on Tensorboard"],"metadata":{"id":"9F4gDKnl5J7y"}},{"cell_type":"code","source":["# !pip install tensorboard\n","%load_ext tensorboard\n","%tensorboard --logdir results/runs"],"metadata":{"id":"KMWmfJsB5Nxn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prompt the newly fine-tuned model\n","* Load and MERGE the LoRA weights with the model weights\n","* Run inference with the same prompt we used to test the pre-trained model"],"metadata":{"id":"F4PrFcvH5Xan"}},{"cell_type":"code","source":["input_text = \"What should I do on a trip to Europe?\"\n","\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    low_cpu_mem_usage=True,\n","    return_dict=True,\n","    torch_dtype=torch.float16,\n","    device_map=device_map,\n",")\n","model = PeftModel.from_pretrained(base_model, new_model)\n","model = model.merge_and_unload()\n","\n","# Reload tokenizer to save it\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""],"metadata":{"id":"9RgeHZcU5Z0x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n","print(input_ids)\n","outputs = model.generate(**input_ids, max_length=128)\n","print(tokenizer.decode(outputs[0]))"],"metadata":{"id":"JiMD63rM8psR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mCN1S0nn-Gga"},"execution_count":null,"outputs":[]}]}